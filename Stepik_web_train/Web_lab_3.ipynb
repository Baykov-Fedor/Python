{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экспортируйте с OSM часть карты, на которой находитесь вы (или любую другую интересную вам). Посчитайте на карте объекты какого-либо типа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачаем карту района по request url'у\n",
    "# И найдем количество школ в этом районе\n",
    "\n",
    "xml = urlopen(\"https://www.openstreetmap.org/api/0.6/map?bbox=30.1955%2C60.003%2C30.2464%2C60.0081\").read().decode('utf-8')\n",
    "soup1 = BeautifulSoup(xml, 'lxml').find_all('node')\n",
    "schools_in_nodes = BeautifulSoup(str(soup1), 'lxml').find_all('tag', k='amenity', v='school')\n",
    "\n",
    "soup2 = BeautifulSoup(xml, 'lxml').find_all('way')\n",
    "schools_in_ways = BeautifulSoup(str(soup2), 'lxml').find_all('tag', k='amenity', v='school')\n",
    "\n",
    "result = schools_in_nodes + schools_in_ways\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = urlopen(\"https://www.openstreetmap.org/api/0.6/map?bbox=30.1955%2C60.003%2C30.2464%2C60.0081\").read().decode('utf-8')\n",
    "\n",
    "parsedxml = xmltodict.parse(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "schools = 0\n",
    "for node in parsedxml['osm']['node']:\n",
    "    flag = 0\n",
    "    if 'tag' in node:\n",
    "        tags = node['tag']\n",
    "        if isinstance(tags, list):\n",
    "            for tag in tags:\n",
    "                if '@k' in tag and tag['@k'] == 'amenity' and tag['@v'] == 'school':\n",
    "                    flag = 1\n",
    "                    schools += 1\n",
    "                if '@k' in tag and tag['@k'] == 'name':\n",
    "                    name = tag['@v']\n",
    "            if flag:\n",
    "                names.append(name)\n",
    "\n",
    "print(schools)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in parsedxml['osm']['way']:\n",
    "    flag = 0\n",
    "    if 'tag' in node:\n",
    "        tags = node['tag']\n",
    "        if isinstance(tags, list):\n",
    "            for tag in tags:\n",
    "                if '@k' in tag and tag['@k'] == 'amenity' and tag['@v'] == 'school':\n",
    "                    flag = 1\n",
    "                    schools += 1\n",
    "                if '@k' in tag and tag['@k'] == 'name':\n",
    "                    name = tag['@v']\n",
    "            if flag:\n",
    "                names.append(name)\n",
    "\n",
    "print(schools)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим два HTML-документа A и B.\n",
    "Из A можно перейти в B за один переход, если в A есть ссылка на B, т. е. внутри A есть тег 'a href=\"B\">', возможно с дополнительными параметрами внутри тега.\n",
    "Из A можно перейти в B за два перехода если существует такой документ C, что из A в C можно перейти за один переход и из C в B можно перейти за один переход.\n",
    "\n",
    "Вашей программе на вход подаются две строки, содержащие url двух документов A и B.\n",
    "Выведите Yes, если из A в B можно перейти за два перехода, иначе выведите No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kinopoisk.ru/lists/top250/\n",
    "https://www.kinopoisk.ru/name/7987/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.kinopoisk.ru\n",
      "https://yandex.ru/support/captcha/\n",
      "['https://www.kinopoisk.ru']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def url_tester():\n",
    "#     A = input('Введите ссылку первого документа: ')\n",
    "#     B = input('Введите ссылку второго документа: ')\n",
    "    \n",
    "    A = 'https://www.kinopoisk.ru/lists/top250/'\n",
    "    B = 'https://www.kinopoisk.ru/name/7987/'\n",
    "    \n",
    "    A_url = urlopen(A).read().decode('utf-8') #качаем первую страничку\n",
    "    A_html = str(A_url) # Делаем из неё строку\n",
    "    A_soup = BeautifulSoup(A_html, 'html.parser') #варим суп\n",
    "    A_origin_url = re.search(r'(.+[.]ru|.+[.]com)', A).group() #вытаскиваем оригинальный url сайта для дальнейшей подставки\n",
    "\n",
    "    B_url = urlopen(B).read().decode('utf-8')\n",
    "    B_html = str(A_url)\n",
    "    B_soup = BeautifulSoup(B_html, 'html.parser')\n",
    "\n",
    "\n",
    "    A_refs = []\n",
    "    for refs in A_soup.find_all('a', href=True): #проходимся по всем ссылкам первой странички\n",
    "        url = refs['href']\n",
    "        print(url)\n",
    "        if url.startswith('/'): #если ссылка относительная \n",
    "            url = A_origin_url + url #то соединяем её с оригинальным url сайта\n",
    "        if url.startswith(A_origin_url): #проверка на то что ссылка не ввёдет на другой сайт\n",
    "            if url == B:\n",
    "                return True\n",
    "            A_refs.append(url)\n",
    "    \n",
    "    \n",
    "    print(A_refs)\n",
    "\n",
    "\n",
    "    for refs in A_refs: #проходимся по ссылкам первой страницы и варим из них супы\n",
    "        C_url = urlopen(refs).read().decode('utf-8')\n",
    "        C_html = str(C_url)\n",
    "        C_soup = BeautifulSoup(C_html, 'html.parser')\n",
    "        \n",
    "        for refs in C_soup.find_all('a', href=True):\n",
    "            url = refs['href']\n",
    "            if url.startswith('/'):\n",
    "                url = A_origin_url + url\n",
    "            if url == B:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "url_tester()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вашей программе на вход подается ссылка на HTML файл.\n",
    "Вам необходимо скачать этот файл, затем найти в нем все ссылки вида <a ... href=\"...\" ... > и вывести список сайтов, на которые есть ссылка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите ссылку на HTML-файл: https://www.plerdy.com/ru/blog/top-50-web-design-inspiration-websites/\n",
      "www.plerdy.com\n",
      "{'ux.stackexchange.com', 'www.uplabs.com', 'www.calltoidea.com', 'www.facebook.com', 'www.thebestdesigns.com', 'www.templatemonster.com', 'www.mockplus.com', 'medium.com', 'alistapart.com', 'www.plerdy.com', 'dribbble.com', 'www.pinterest.com', 'www.uxdesignedge.com', 'www.creativebloq.com', 'www.pentagram.com', 'a.plerdy.com', 'play.google.com', 'www.flickr.com', 'book.com', 'twitter.com', 'uimovement.com', 'itunes.apple.com', 'abduzeedo.com', 'www.youtube.com', 'thegreatdiscontent.com', 'www.linkedin.com', 'uxmag.com', 'www.siteinspire.com', 'www.awwwards.com', 'onepagelove.com', 'collectui.com', 'cssnectar.com', 'pttrns.com', '99designs.com', 'www.smashingmagazine.com'}\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "def url_tester():\n",
    "    A = input('Введите ссылку на HTML-файл: ')\n",
    "   \n",
    "    A_url = urlopen(A).read().decode('utf-8')\n",
    "    A_html = str(A_url)\n",
    "    A_soup = BeautifulSoup(A_html, 'html.parser')\n",
    "    \n",
    "    A_origin_url = re.search(r'([\\w.]+[.]ru|[\\w.]+[.]com)', A).group()\n",
    "    print(A_origin_url)\n",
    "    \n",
    "    A_refs = set()\n",
    "    for refs in A_soup.find_all('a', href=True):\n",
    "        if '../' in refs['href']:\n",
    "            continue \n",
    "        A_origin_url = re.search(r'([\\w.]+[.]ru|[\\w.]+[.]com)', refs['href'])\n",
    "        if A_origin_url:\n",
    "            A_origin_url = A_origin_url.group()\n",
    "            A_refs.add(A_origin_url)\n",
    "    print(A_refs)\n",
    "    print(len(A_refs))\n",
    "url_tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://pastebin.com/raw/7543p0ns\n",
    "https://www.plerdy.com/ru/blog/top-50-web-design-inspiration-websites/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
