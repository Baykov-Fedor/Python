{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 4: topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе мы попытаемся обучить LDA-модель topic-моделингу на двух принципиально различных корпусах. \n",
    "\n",
    "В первой части вы познакомитесь с новыми возможностями библиотеки gensim, а также с возможностями парсинга в языке Python. Во второй части вам предстоит самостоятельно обучить LDA-модель и оценить качество её работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: topic modeling уровня /b/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Краеугольным камнем в машинном обучений в целом, и в NLP в частности, является выбор датасетов. Доселе мы использовали только стандартные, многократно обкатанные датасеты, но сегодня попробуем собрать свой. Практика работы с сырыми, необработанными данными весьма полезно! Заодно изучим возможности парсеров в Питоне.\n",
    "\n",
    "Давайте напишем парсер, собирающий информацию о сообщения с русскомязычного анононимного форума (имиджборды) \"Двач\" (\"Сосач\", \"Хиккач\", если вам угодно). Двач, как и всякая имиджборда разделён на разделы (доски, борды), посвященные различным тематикам -- аниме, видеоигры, литература, религия... Каждая доска состоит из тем (тредов, топиков), которые создаются анонимными (при их желании) пользователями. Каждый тред посвящен обсуждению какого-то конкретного вопроса.\n",
    "\n",
    "У некоторых разделов есть раздел архив, располагается он по адресу https://2ch.hk/(название раздела)/arch/, например для раздела музыка -- https://2ch.hk/mu/arch/. Если у вас есть минимальные навыки в языке html, а также если вы изучили документацию встроенного класса HTMLParser, то вам будет несложно написать два парсера.\n",
    "\n",
    "Первый парсер (ArchiveParser) парсит HTML-страницу архива доски, вытягивает из неё ссылки на заархивированные треды, и скармливает их второму парсеру.\n",
    "\n",
    "Второй парсер (ThreadParser) парсит HTML-страницу заархивированного треда, вытягивает из неё сообщения, складывает их вместе и собирает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "from html.parser import HTMLParser\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def get_value_by_key(attrs, key):\n",
    "    for (k, v) in attrs:\n",
    "        if(k == key):\n",
    "            \n",
    "            return v;\n",
    "    return None\n",
    "\n",
    "class ArchiveParser(HTMLParser):\n",
    "    flag = False\n",
    "    threads = []\n",
    "    limit = 200\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if(self.limit > 0):\n",
    "            if(tag == 'div'):\n",
    "                cl = get_value_by_key(attrs, 'class')\n",
    "                if (cl == 'box-data'):\n",
    "                    self.flag = True;\n",
    "            if(self.flag == True and tag == 'a'):\n",
    "                href = get_value_by_key(attrs, 'href')\n",
    "                if(len(href)>20):\n",
    "                    print(href)\n",
    "                    print(self.limit)\n",
    "                    thread = parse_thread('https://2ch.hk' + href)\n",
    "                    if(len(thread) > 10):\n",
    "                        self.threads.append(thread)\n",
    "                        self.limit = self.limit - 1\n",
    "                    thread = []\n",
    "        \n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if(tag == 'div'):\n",
    "            self.flag = False;\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        1+1\n",
    "        \n",
    "    def get_threads(self):\n",
    "        return self.threads\n",
    "    \n",
    "    def clean(self):\n",
    "        self.threads = []\n",
    "        \n",
    "parser = ArchiveParser()\n",
    "\n",
    "def parse_archive(board = '/b/', page_number = 0):\n",
    "    lines = []\n",
    "    link = 'https://2ch.hk' + board + 'arch/' + str(page_number) +'.html'\n",
    "    print(link)\n",
    "    parser.limit = 100\n",
    "    url = urllib.request.urlopen(link)\n",
    "    for line in url.readlines():\n",
    "        lines.append(line.decode('utf-8'))\n",
    "    for line in lines:\n",
    "        parser.feed(line)\n",
    "    res = parser.get_threads()\n",
    "    parser.clean()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadParser(HTMLParser):\n",
    "    flag = False\n",
    "    message = []\n",
    "    messages = []\n",
    "            \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if(tag == 'blockquote'):\n",
    "            self.flag = True;\n",
    "            self.message = []\n",
    "        \n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if(tag == 'blockquote'):\n",
    "            self.flag = False\n",
    "            if(self.message != []):\n",
    "                self.messages.append(self.message)\n",
    "            self.message = []\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if(self.flag):\n",
    "            self.message.extend(simple_preprocess(data))\n",
    "            \n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "    \n",
    "    def clear_messages(self):\n",
    "        flag = False\n",
    "        self.message = []\n",
    "        self.messages = []\n",
    "\n",
    "t_parser = ThreadParser()\n",
    "\n",
    "def parse_thread (link):\n",
    "    url = urllib.request.urlopen(link)\n",
    "    lines = []\n",
    "    for line in url.readlines():\n",
    "        lines.append(line.decode('utf-8', errors='ignore'))\n",
    "    for line in lines:\n",
    "        t_parser.feed(line)\n",
    "    res = t_parser.get_messages()\n",
    "    t_parser.clear_messages()\n",
    "    #print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весьма много кода, верно? Если не потерялись, могли заметить функцию parse_archive, которая парсит страницу архива по доске и номеру страницы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание.}$\n",
    "Давайте применим её к каким-нибудь доскам. Выберите две доски двача, имеющие архив и скачайте архивы функцией parse_archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://2ch.hk/fiz/arch/0.html\n",
      "/fiz/arch/2016-04-29/res/828107.html\n",
      "100\n",
      "/fiz/arch/2016-05-02/res/827737.html\n",
      "100\n",
      "/fiz/arch/2016-09-04/res/827560.html\n",
      "99\n",
      "/fiz/arch/2016-05-09/res/827545.html\n",
      "98\n",
      "/fiz/arch/2016-04-30/res/826831.html\n",
      "97\n",
      "/fiz/arch/2016-05-11/res/826655.html\n",
      "96\n",
      "/fiz/arch/2016-04-28/res/826506.html\n",
      "95\n",
      "/fiz/arch/2016-05-01/res/826467.html\n",
      "95\n",
      "/fiz/arch/2016-04-28/res/826451.html\n",
      "94\n",
      "/fiz/arch/2016-04-29/res/826378.html\n",
      "93\n",
      "/fiz/arch/2016-05-07/res/826266.html\n",
      "92\n",
      "/fiz/arch/2016-05-29/res/826087.html\n",
      "91\n",
      "/fiz/arch/2016-05-01/res/825996.html\n",
      "90\n",
      "/fiz/arch/2016-05-08/res/825948.html\n",
      "89\n",
      "/fiz/arch/2016-04-29/res/825832.html\n",
      "88\n",
      "/fiz/arch/2016-05-16/res/825684.html\n",
      "87\n",
      "/fiz/arch/2016-05-15/res/825479.html\n",
      "87\n",
      "/fiz/arch/2016-04-30/res/825287.html\n",
      "86\n",
      "/fiz/arch/2016-05-09/res/825230.html\n",
      "86\n",
      "/fiz/arch/2016-04-28/res/825199.html\n",
      "85\n",
      "/fiz/arch/2016-04-28/res/825193.html\n",
      "85\n",
      "/fiz/arch/2016-04-28/res/825183.html\n",
      "84\n",
      "/fiz/arch/2016-08-13/res/825133.html\n",
      "83\n",
      "/fiz/arch/2016-04-29/res/824943.html\n",
      "82\n",
      "/fiz/arch/2016-04-27/res/824834.html\n",
      "81\n",
      "/fiz/arch/2016-04-29/res/824530.html\n",
      "81\n",
      "/fiz/arch/2016-04-26/res/824481.html\n",
      "80\n",
      "/fiz/arch/2016-04-28/res/824379.html\n",
      "79\n",
      "/fiz/arch/2016-04-29/res/824349.html\n",
      "78\n",
      "/fiz/arch/2016-04-26/res/824338.html\n",
      "77\n",
      "/fiz/arch/2016-04-30/res/824036.html\n",
      "77\n",
      "/fiz/arch/2016-04-25/res/823970.html\n",
      "76\n",
      "/fiz/arch/2016-04-25/res/823897.html\n",
      "76\n",
      "/fiz/arch/2016-05-03/res/823526.html\n",
      "76\n",
      "/fiz/arch/2016-04-26/res/823471.html\n",
      "75\n",
      "/fiz/arch/2016-04-24/res/823383.html\n",
      "74\n",
      "/fiz/arch/2016-06-18/res/823289.html\n",
      "74\n",
      "/fiz/arch/2016-04-30/res/823179.html\n",
      "73\n",
      "/fiz/arch/2016-04-25/res/822937.html\n",
      "73\n",
      "/fiz/arch/2016-04-29/res/822889.html\n",
      "72\n",
      "/fiz/arch/2016-04-23/res/822649.html\n",
      "71\n",
      "/fiz/arch/2016-04-25/res/822591.html\n",
      "70\n",
      "/fiz/arch/2016-05-02/res/822409.html\n",
      "70\n",
      "/fiz/arch/2016-04-23/res/822343.html\n",
      "69\n",
      "/fiz/arch/2016-04-29/res/822307.html\n",
      "68\n",
      "/fiz/arch/2016-04-28/res/822294.html\n",
      "67\n",
      "/fiz/arch/2016-03-27/res/822226.html\n",
      "66\n",
      "/fiz/arch/2016-05-11/res/822164.html\n",
      "65\n",
      "/fiz/arch/2016-04-23/res/822069.html\n",
      "64\n",
      "/fiz/arch/2016-03-26/res/821807.html\n",
      "64\n",
      "/fiz/arch/2016-05-08/res/821613.html\n",
      "63\n",
      "/fiz/arch/2016-05-09/res/821474.html\n",
      "63\n",
      "/fiz/arch/2016-04-26/res/821326.html\n",
      "62\n",
      "/fiz/arch/2016-03-25/res/821232.html\n",
      "61\n",
      "/fiz/arch/2016-03-25/res/821061.html\n",
      "61\n",
      "/fiz/arch/2016-04-23/res/820961.html\n",
      "61\n",
      "/fiz/arch/2016-05-17/res/820792.html\n",
      "60\n",
      "/fiz/arch/2016-04-25/res/820693.html\n",
      "59\n",
      "/fiz/arch/2016-05-13/res/820455.html\n",
      "58\n",
      "/fiz/arch/2016-04-27/res/820408.html\n",
      "57\n",
      "/fiz/arch/2016-05-29/res/820140.html\n",
      "56\n",
      "/fiz/arch/2016-05-08/res/820031.html\n",
      "55\n",
      "/fiz/arch/2016-03-24/res/819971.html\n",
      "54\n",
      "/fiz/arch/2016-03-24/res/819919.html\n",
      "53\n",
      "/fiz/arch/2016-04-28/res/819754.html\n",
      "53\n",
      "/fiz/arch/2016-03-23/res/819448.html\n",
      "52\n",
      "/fiz/arch/2016-03-23/res/819306.html\n",
      "52\n",
      "/fiz/arch/2016-03-23/res/819296.html\n",
      "51\n",
      "/fiz/arch/2016-03-23/res/819271.html\n",
      "50\n",
      "/fiz/arch/2016-03-23/res/819218.html\n",
      "50\n",
      "/fiz/arch/2016-03-23/res/819084.html\n",
      "50\n",
      "/fiz/arch/2016-04-27/res/819080.html\n",
      "50\n",
      "/fiz/arch/2016-05-15/res/818955.html\n",
      "49\n",
      "/fiz/arch/2016-03-22/res/818894.html\n",
      "48\n",
      "/fiz/arch/2016-03-22/res/818809.html\n",
      "47\n",
      "/fiz/arch/2016-07-01/res/818807.html\n",
      "47\n",
      "/fiz/arch/2016-03-22/res/818788.html\n",
      "46\n",
      "/fiz/arch/2016-03-22/res/818786.html\n",
      "45\n",
      "/fiz/arch/2016-03-22/res/818720.html\n",
      "45\n",
      "/fiz/arch/2016-04-23/res/818700.html\n",
      "44\n",
      "/fiz/arch/2016-04-27/res/818613.html\n",
      "43\n",
      "/fiz/arch/2016-03-22/res/818595.html\n",
      "42\n",
      "/fiz/arch/2016-05-14/res/818540.html\n",
      "41\n",
      "/fiz/arch/2016-03-22/res/818425.html\n",
      "40\n",
      "/fiz/arch/2016-03-22/res/818327.html\n",
      "40\n",
      "/fiz/arch/2016-04-27/res/818317.html\n",
      "39\n",
      "/fiz/arch/2016-05-13/res/818239.html\n",
      "38\n",
      "/fiz/arch/2016-03-22/res/818198.html\n",
      "37\n",
      "/fiz/arch/2016-05-03/res/818171.html\n",
      "36\n",
      "/fiz/arch/2016-05-17/res/818067.html\n",
      "35\n",
      "/fiz/arch/2016-04-24/res/818043.html\n",
      "34\n",
      "/fiz/arch/2016-03-21/res/818020.html\n",
      "33\n",
      "/fiz/arch/2016-03-21/res/818011.html\n",
      "32\n",
      "/fiz/arch/2016-03-21/res/818008.html\n",
      "32\n",
      "/fiz/arch/2016-03-21/res/817775.html\n",
      "32\n",
      "/fiz/arch/2016-03-21/res/817665.html\n",
      "32\n",
      "/fiz/arch/2016-05-04/res/817464.html\n",
      "31\n",
      "/fiz/arch/2016-03-20/res/817375.html\n",
      "30\n",
      "/fiz/arch/2016-03-20/res/817213.html\n",
      "29\n",
      "/fiz/arch/2016-03-20/res/817172.html\n",
      "29\n",
      "/fiz/arch/2016-03-20/res/817021.html\n",
      "28\n",
      "/fiz/arch/2016-03-19/res/816941.html\n",
      "27\n",
      "/fiz/arch/2016-03-19/res/816756.html\n",
      "26\n",
      "/fiz/arch/2016-03-19/res/816656.html\n",
      "26\n",
      "/fiz/arch/2016-03-19/res/816549.html\n",
      "25\n",
      "/fiz/arch/2016-03-19/res/816533.html\n",
      "24\n",
      "/fiz/arch/2016-03-19/res/816450.html\n",
      "24\n",
      "/fiz/arch/2016-03-18/res/816273.html\n",
      "23\n",
      "/fiz/arch/2016-03-18/res/816142.html\n",
      "23\n",
      "/fiz/arch/2016-03-18/res/816123.html\n",
      "22\n",
      "/fiz/arch/2016-03-18/res/816120.html\n",
      "22\n",
      "/fiz/arch/2016-03-18/res/815906.html\n",
      "21\n",
      "/fiz/arch/2016-03-17/res/815410.html\n",
      "20\n",
      "/fiz/arch/2016-03-17/res/815399.html\n",
      "19\n",
      "/fiz/arch/2016-03-17/res/815340.html\n",
      "19\n",
      "/fiz/arch/2016-03-17/res/815308.html\n",
      "18\n",
      "/fiz/arch/2016-03-17/res/815229.html\n",
      "18\n",
      "/fiz/arch/2016-03-17/res/815155.html\n",
      "17\n",
      "/fiz/arch/2016-04-29/res/815091.html\n",
      "16\n",
      "/fiz/arch/2016-03-16/res/814758.html\n",
      "15\n",
      "/fiz/arch/2016-03-16/res/814538.html\n",
      "14\n",
      "/fiz/arch/2016-03-16/res/814396.html\n",
      "14\n",
      "/fiz/arch/2016-03-16/res/814361.html\n",
      "13\n",
      "/fiz/arch/2016-05-17/res/814219.html\n",
      "12\n",
      "/fiz/arch/2016-04-28/res/814218.html\n",
      "11\n",
      "/fiz/arch/2016-05-02/res/814041.html\n",
      "10\n",
      "/fiz/arch/2016-07-18/res/814036.html\n",
      "9\n",
      "/fiz/arch/2016-03-16/res/813957.html\n",
      "8\n",
      "/fiz/arch/2016-03-15/res/813893.html\n",
      "8\n",
      "/fiz/arch/2016-04-25/res/813872.html\n",
      "7\n",
      "/fiz/arch/2016-03-15/res/813852.html\n",
      "6\n",
      "/fiz/arch/2016-03-15/res/813661.html\n",
      "6\n",
      "/fiz/arch/2016-03-15/res/813474.html\n",
      "6\n",
      "/fiz/arch/2016-05-29/res/813363.html\n",
      "6\n",
      "/fiz/arch/2016-03-14/res/812886.html\n",
      "5\n",
      "/fiz/arch/2016-03-14/res/812872.html\n",
      "4\n",
      "/fiz/arch/2016-03-14/res/812845.html\n",
      "3\n",
      "/fiz/arch/2016-03-14/res/812730.html\n",
      "2\n",
      "/fiz/arch/2016-03-14/res/812518.html\n",
      "1\n",
      "https://2ch.hk/re/arch/0.html\n",
      "/re/arch/2016-07-11/res/357867.html\n",
      "100\n",
      "/re/arch/2016-06-28/res/357546.html\n",
      "99\n",
      "/re/arch/2016-06-27/res/357492.html\n",
      "98\n",
      "/re/arch/2016-06-26/res/357485.html\n",
      "98\n",
      "/re/arch/2016-06-27/res/357484.html\n",
      "98\n",
      "/re/arch/2016-06-25/res/357379.html\n",
      "98\n",
      "/re/arch/2016-06-30/res/357372.html\n",
      "98\n",
      "/re/arch/2016-06-29/res/357310.html\n",
      "97\n",
      "/re/arch/2016-11-27/res/357263.html\n",
      "96\n",
      "/re/arch/2016-06-26/res/357190.html\n",
      "95\n",
      "/re/arch/2016-06-27/res/357156.html\n",
      "94\n",
      "/re/arch/2016-07-04/res/357033.html\n",
      "93\n",
      "/re/arch/2016-06-25/res/356956.html\n",
      "92\n",
      "/re/arch/2016-06-25/res/356934.html\n",
      "92\n",
      "/re/arch/2016-06-26/res/356829.html\n",
      "92\n",
      "/re/arch/2016-08-18/res/356658.html\n",
      "91\n",
      "/re/arch/2016-06-30/res/356613.html\n",
      "90\n",
      "/re/arch/2016-08-10/res/356608.html\n",
      "89\n",
      "/re/arch/2016-08-11/res/356596.html\n",
      "88\n",
      "/re/arch/2016-06-25/res/356595.html\n",
      "87\n",
      "/re/arch/2016-06-24/res/356271.html\n",
      "86\n",
      "/re/arch/2016-06-25/res/356222.html\n",
      "85\n",
      "/re/arch/2016-06-25/res/356216.html\n",
      "84\n",
      "/re/arch/2016-06-24/res/356167.html\n",
      "83\n",
      "/re/arch/2016-06-24/res/355983.html\n",
      "83\n",
      "/re/arch/2016-06-29/res/355572.html\n",
      "82\n",
      "/re/arch/2016-06-22/res/355435.html\n",
      "81\n",
      "/re/arch/2016-06-23/res/355403.html\n",
      "80\n",
      "/re/arch/2016-08-03/res/355143.html\n",
      "79\n",
      "/re/arch/2016-08-23/res/355056.html\n",
      "78\n",
      "/re/arch/2016-06-22/res/355021.html\n",
      "77\n",
      "/re/arch/2016-06-23/res/354351.html\n",
      "76\n",
      "/re/arch/2016-06-30/res/354220.html\n",
      "75\n",
      "/re/arch/2016-06-19/res/354154.html\n",
      "74\n",
      "/re/arch/2016-06-22/res/353886.html\n",
      "74\n",
      "/re/arch/2016-06-17/res/353591.html\n",
      "73\n",
      "/re/arch/2016-06-20/res/353543.html\n",
      "73\n",
      "/re/arch/2016-07-02/res/353520.html\n",
      "72\n",
      "/re/arch/2016-06-17/res/353288.html\n",
      "71\n",
      "/re/arch/2016-07-18/res/353246.html\n",
      "70\n",
      "/re/arch/2016-09-17/res/352368.html\n",
      "69\n",
      "/re/arch/2016-06-22/res/352228.html\n",
      "68\n",
      "/re/arch/2016-06-16/res/352181.html\n",
      "67\n",
      "/re/arch/2016-06-18/res/352143.html\n",
      "66\n",
      "/re/arch/2016-08-11/res/352064.html\n",
      "65\n",
      "/re/arch/2016-06-27/res/351863.html\n",
      "64\n",
      "/re/arch/2016-07-12/res/351706.html\n",
      "64\n",
      "/re/arch/2019-01-09/res/351436.html\n",
      "63\n",
      "/re/arch/2016-07-03/res/351158.html\n",
      "63\n",
      "/re/arch/2016-06-13/res/351130.html\n",
      "62\n",
      "/re/arch/2016-06-15/res/351093.html\n",
      "62\n",
      "/re/arch/2016-06-24/res/350989.html\n",
      "61\n",
      "/re/arch/2016-06-16/res/350968.html\n",
      "60\n",
      "/re/arch/2016-07-07/res/350855.html\n",
      "59\n",
      "/re/arch/2016-07-18/res/350759.html\n",
      "58\n",
      "/re/arch/2016-06-13/res/350525.html\n",
      "57\n",
      "/re/arch/2016-07-13/res/350381.html\n",
      "56\n",
      "/re/arch/2016-06-14/res/350196.html\n",
      "55\n",
      "/re/arch/2016-06-11/res/350156.html\n",
      "54\n",
      "/re/arch/2016-06-14/res/350047.html\n",
      "54\n",
      "/re/arch/2016-06-13/res/350042.html\n",
      "54\n",
      "/re/arch/2016-07-31/res/349844.html\n",
      "54\n",
      "/re/arch/2016-07-07/res/349692.html\n",
      "53\n",
      "/re/arch/2016-07-18/res/349670.html\n",
      "52\n",
      "/re/arch/2016-06-08/res/349556.html\n",
      "51\n",
      "/re/arch/2016-06-20/res/349512.html\n",
      "51\n",
      "/re/arch/2016-06-10/res/349407.html\n",
      "50\n",
      "/re/arch/2016-06-07/res/349376.html\n",
      "49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/re/arch/2016-06-08/res/349342.html\n",
      "49\n",
      "/re/arch/2016-06-13/res/349286.html\n",
      "49\n",
      "/re/arch/2016-07-21/res/349266.html\n",
      "48\n",
      "/re/arch/2016-06-13/res/349225.html\n",
      "47\n",
      "/re/arch/2016-06-09/res/349129.html\n",
      "46\n",
      "/re/arch/2016-06-04/res/349111.html\n",
      "45\n",
      "/re/arch/2016-06-03/res/348969.html\n",
      "45\n",
      "/re/arch/2016-06-04/res/348958.html\n",
      "45\n",
      "/re/arch/2016-06-04/res/348924.html\n",
      "44\n",
      "/re/arch/2016-06-03/res/348864.html\n",
      "44\n",
      "/re/arch/2016-06-15/res/348731.html\n",
      "44\n",
      "/re/arch/2016-06-28/res/348705.html\n",
      "44\n",
      "/re/arch/2016-09-04/res/348643.html\n",
      "43\n",
      "/re/arch/2016-06-20/res/348602.html\n",
      "42\n",
      "/re/arch/2016-06-01/res/348492.html\n",
      "41\n",
      "/re/arch/2016-06-01/res/348484.html\n",
      "41\n",
      "/re/arch/2016-06-14/res/348466.html\n",
      "41\n",
      "/re/arch/2016-07-20/res/348387.html\n",
      "40\n",
      "/re/arch/2016-06-20/res/348385.html\n",
      "39\n",
      "/re/arch/2016-05-31/res/348350.html\n",
      "38\n",
      "/re/arch/2016-06-07/res/348234.html\n",
      "38\n",
      "/re/arch/2016-05-29/res/348197.html\n",
      "37\n",
      "/re/arch/2016-06-05/res/348191.html\n",
      "37\n",
      "/re/arch/2016-06-10/res/347964.html\n",
      "36\n",
      "/re/arch/2016-06-06/res/347936.html\n",
      "35\n",
      "/re/arch/2016-06-14/res/347794.html\n",
      "34\n",
      "/re/arch/2016-08-05/res/347724.html\n",
      "33\n",
      "/re/arch/2016-05-30/res/347616.html\n",
      "32\n",
      "/re/arch/2016-06-28/res/347604.html\n",
      "31\n",
      "/re/arch/2016-05-28/res/347564.html\n",
      "30\n",
      "/re/arch/2019-07-04/res/347531.html\n",
      "29\n",
      "/re/arch/2016-05-25/res/347461.html\n",
      "29\n",
      "/re/arch/2016-05-29/res/347252.html\n",
      "28\n",
      "/re/arch/2016-05-30/res/347003.html\n",
      "27\n",
      "/re/arch/2016-05-25/res/346954.html\n",
      "26\n",
      "/re/arch/2016-06-18/res/346936.html\n",
      "25\n",
      "/re/arch/2016-05-23/res/346879.html\n",
      "25\n",
      "/re/arch/2016-05-25/res/346819.html\n",
      "25\n",
      "/re/arch/2016-07-21/res/346790.html\n",
      "25\n",
      "/re/arch/2016-06-19/res/346502.html\n",
      "24\n",
      "/re/arch/2016-05-20/res/346424.html\n",
      "23\n",
      "/re/arch/2016-06-03/res/346417.html\n",
      "23\n",
      "/re/arch/2016-05-22/res/346208.html\n",
      "22\n",
      "/re/arch/2016-05-28/res/346187.html\n",
      "21\n",
      "/re/arch/2016-06-18/res/346019.html\n",
      "20\n",
      "/re/arch/2016-05-22/res/345955.html\n",
      "19\n",
      "/re/arch/2016-05-25/res/345807.html\n",
      "18\n",
      "/re/arch/2016-05-16/res/345519.html\n",
      "17\n",
      "/re/arch/2016-08-11/res/345484.html\n",
      "17\n",
      "/re/arch/2016-05-27/res/345278.html\n",
      "16\n",
      "/re/arch/2016-05-28/res/345274.html\n",
      "15\n",
      "/re/arch/2016-10-08/res/345062.html\n",
      "14\n",
      "/re/arch/2016-05-12/res/345022.html\n",
      "13\n",
      "/re/arch/2016-08-06/res/344981.html\n",
      "13\n",
      "/re/arch/2016-05-24/res/344834.html\n",
      "12\n",
      "/re/arch/2016-05-20/res/344519.html\n",
      "11\n",
      "/re/arch/2016-05-18/res/344493.html\n",
      "10\n",
      "/re/arch/2016-05-11/res/344460.html\n",
      "9\n",
      "/re/arch/2016-05-12/res/344458.html\n",
      "9\n",
      "/re/arch/2016-07-06/res/344394.html\n",
      "8\n",
      "/re/arch/2016-05-20/res/344332.html\n",
      "7\n",
      "/re/arch/2016-05-25/res/344267.html\n",
      "6\n",
      "/re/arch/2016-05-20/res/344235.html\n",
      "6\n",
      "/re/arch/2016-06-09/res/343937.html\n",
      "5\n",
      "/re/arch/2016-05-25/res/343788.html\n",
      "4\n",
      "/re/arch/2016-05-10/res/343773.html\n",
      "3\n",
      "/re/arch/2016-05-08/res/343725.html\n",
      "3\n",
      "/re/arch/2016-06-22/res/343705.html\n",
      "2\n",
      "/re/arch/2016-05-09/res/343702.html\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "boards = ['/fiz/', '/re/'] \n",
    "#TODO: напишите название досок в формате /'доска'/, например /mu/ для Музыки\n",
    "threads_by_topic = [parse_archive(board=board) for board in boards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данны на тренировочые и тестовые. Пусть каждый десятый тред попадает в тест-сет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сап',\n",
       " 'физач',\n",
       " 'есть',\n",
       " 'один',\n",
       " 'больной',\n",
       " 'скиннифэт',\n",
       " 'кароче',\n",
       " 'меня',\n",
       " 'была',\n",
       " 'операция',\n",
       " 'по',\n",
       " 'поводу',\n",
       " 'иссечение',\n",
       " 'липомы',\n",
       " 'на',\n",
       " 'уровне',\n",
       " 'иссечение',\n",
       " 'сакральной',\n",
       " 'кисты',\n",
       " 'на',\n",
       " 'уровне',\n",
       " 'на',\n",
       " 'последнем',\n",
       " 'мрт',\n",
       " 'все',\n",
       " 'норм',\n",
       " 'но',\n",
       " 'врач',\n",
       " 'сказал',\n",
       " 'что',\n",
       " 'можно',\n",
       " 'заниматься',\n",
       " 'только',\n",
       " 'без',\n",
       " 'осевой',\n",
       " 'нагрузки',\n",
       " 'на',\n",
       " 'позвоночник',\n",
       " 'вообще',\n",
       " 'избегать',\n",
       " 'нагрузок',\n",
       " 'на',\n",
       " 'поясницу',\n",
       " 'впринципе',\n",
       " 'позвоночник',\n",
       " 'особых',\n",
       " 'поясните',\n",
       " 'боги',\n",
       " 'физача',\n",
       " 'апполоны',\n",
       " 'во',\n",
       " 'плоти',\n",
       " 'как',\n",
       " 'мне',\n",
       " 'можно',\n",
       " 'заниматься',\n",
       " 'какими',\n",
       " 'упражнениями',\n",
       " 'чего',\n",
       " 'начинать',\n",
       " 'бамп',\n",
       " 'помогите',\n",
       " 'op',\n",
       " 'зал',\n",
       " 'без',\n",
       " 'осевой',\n",
       " 'нагрузки',\n",
       " 'на',\n",
       " 'позвоночник',\n",
       " 'это',\n",
       " 'невозможно',\n",
       " 'врач',\n",
       " 'пиздит',\n",
       " 'заниматься',\n",
       " 'можно',\n",
       " 'начинать',\n",
       " 'надо',\n",
       " 'веса',\n",
       " 'которым',\n",
       " 'ничего',\n",
       " 'не',\n",
       " 'болит',\n",
       " 'шти',\n",
       " 'ты',\n",
       " 'из',\n",
       " 'ниво',\n",
       " 'инвалида',\n",
       " 'сделаишь',\n",
       " 'эмммм',\n",
       " 'ну',\n",
       " 'жим',\n",
       " 'лёжа',\n",
       " 'это',\n",
       " 'же',\n",
       " 'тип',\n",
       " 'не',\n",
       " 'осевая',\n",
       " 'op',\n",
       " 'на',\n",
       " 'тренажерах',\n",
       " 'каких',\n",
       " 'подробнее',\n",
       " 'бро',\n",
       " 'кароче',\n",
       " 'кто',\n",
       " 'норм',\n",
       " 'ответит',\n",
       " 'развернуто',\n",
       " 'тому',\n",
       " 'кину',\n",
       " 'рубчинских',\n",
       " 'на',\n",
       " 'палку',\n",
       " 'все',\n",
       " 'упражнения',\n",
       " 'на',\n",
       " 'турнике',\n",
       " 'брусьях',\n",
       " 'такие',\n",
       " 'где',\n",
       " 'поясница',\n",
       " 'касается',\n",
       " 'скамьи',\n",
       " 'для',\n",
       " 'тебя',\n",
       " 'ну',\n",
       " 'если',\n",
       " 'жать',\n",
       " 'по',\n",
       " 'технике',\n",
       " 'то',\n",
       " 'есть',\n",
       " 'использовать',\n",
       " 'leg',\n",
       " 'drive',\n",
       " 'хз',\n",
       " 'как',\n",
       " 'по',\n",
       " 'русски',\n",
       " 'то',\n",
       " 'будет',\n",
       " 'осевая',\n",
       " 'просто',\n",
       " 'берёшь',\n",
       " 'заменяешь',\n",
       " 'все',\n",
       " 'упражнения',\n",
       " 'осевой',\n",
       " 'нагрузкой',\n",
       " 'на',\n",
       " 'поясницу',\n",
       " 'приседания',\n",
       " 'со',\n",
       " 'штангой',\n",
       " 'становая',\n",
       " 'жим',\n",
       " 'стоя',\n",
       " 'тяга',\n",
       " 'штанги',\n",
       " 'наклоне',\n",
       " 'тренажерами',\n",
       " 'присед',\n",
       " 'жим',\n",
       " 'ногами',\n",
       " 'тренажёре',\n",
       " 'под',\n",
       " 'углом',\n",
       " 'не',\n",
       " 'путай',\n",
       " 'гакк',\n",
       " 'приседаниями',\n",
       " 'там',\n",
       " 'нагрузка',\n",
       " 'на',\n",
       " 'спину',\n",
       " 'идёт',\n",
       " 'становая',\n",
       " 'гиперэкстензии',\n",
       " 'или',\n",
       " 'обратные',\n",
       " 'гиперэкстензии',\n",
       " 'делай',\n",
       " 'многоповторку',\n",
       " 'маленькими',\n",
       " 'весами',\n",
       " 'или',\n",
       " 'вовсе',\n",
       " 'без',\n",
       " 'отягощений',\n",
       " 'на',\n",
       " 'первое',\n",
       " 'время',\n",
       " 'спина',\n",
       " 'спасибо',\n",
       " 'скажет',\n",
       " 'жим',\n",
       " 'стоя',\n",
       " 'не',\n",
       " 'знаю',\n",
       " 'как',\n",
       " 'тренажер',\n",
       " 'называется',\n",
       " 'но',\n",
       " 'его',\n",
       " 'суть',\n",
       " 'том',\n",
       " 'чтобы',\n",
       " 'жать',\n",
       " 'вес',\n",
       " 'вверх',\n",
       " 'не',\n",
       " 'перепутаешь',\n",
       " 'тяга',\n",
       " 'штанги',\n",
       " 'наклоне',\n",
       " 'подтягивания',\n",
       " 'тяги',\n",
       " 'блоков',\n",
       " 'хотя',\n",
       " 'жим',\n",
       " 'стоя',\n",
       " 'хрен',\n",
       " 'чем',\n",
       " 'заменишь',\n",
       " 'даже',\n",
       " 'тренажере',\n",
       " 'нагрузка',\n",
       " 'на',\n",
       " 'позвоночник',\n",
       " 'есть',\n",
       " 'не',\n",
       " 'малая',\n",
       " 'остаются',\n",
       " 'только',\n",
       " 'упражнения',\n",
       " 'со',\n",
       " 'своим',\n",
       " 'весом',\n",
       " 'конкретно',\n",
       " 'отжимания',\n",
       " 'стойке',\n",
       " 'на',\n",
       " 'руках',\n",
       " 'спс',\n",
       " 'бро',\n",
       " 'пиши',\n",
       " 'палку',\n",
       " 'палку',\n",
       " 'paypal',\n",
       " 'жим',\n",
       " 'стоя',\n",
       " 'не',\n",
       " 'знаю',\n",
       " 'как',\n",
       " 'тренажер',\n",
       " 'называется',\n",
       " 'но',\n",
       " 'его',\n",
       " 'суть',\n",
       " 'том',\n",
       " 'чтобы',\n",
       " 'жать',\n",
       " 'вес',\n",
       " 'вверх',\n",
       " 'не',\n",
       " 'перепутаешь',\n",
       " 'атятя',\n",
       " 'убийца',\n",
       " 'даже',\n",
       " 'не',\n",
       " 'знаю',\n",
       " 'кто',\n",
       " 'тут',\n",
       " 'троль',\n",
       " 'оп',\n",
       " 'или',\n",
       " 'же',\n",
       " 'весь',\n",
       " 'тред',\n",
       " 'двачую',\n",
       " 'вопрос',\n",
       " 'опа',\n",
       " 'проблема',\n",
       " 'как',\n",
       " 'раз',\n",
       " 'том',\n",
       " 'чем',\n",
       " 'загрузить',\n",
       " 'плечи',\n",
       " 'трапеции',\n",
       " 'остальным',\n",
       " 'давно',\n",
       " 'разобрался',\n",
       " 'от',\n",
       " 'жима',\n",
       " 'ногами',\n",
       " 'меня',\n",
       " 'например',\n",
       " 'поясница',\n",
       " 'полбаливает',\n",
       " 'op',\n",
       " 'занимайся',\n",
       " 'без',\n",
       " 'сильной',\n",
       " 'осевой',\n",
       " 'нагрузки',\n",
       " 'то',\n",
       " 'есть',\n",
       " 'без',\n",
       " 'упражнений',\n",
       " 'со',\n",
       " 'штангой',\n",
       " 'гантелями',\n",
       " 'можно',\n",
       " 'жим',\n",
       " 'штанги',\n",
       " 'горизонтальный',\n",
       " 'делай',\n",
       " 'ногами',\n",
       " 'на',\n",
       " 'скамье',\n",
       " 'ясен',\n",
       " 'хуй',\n",
       " 'все',\n",
       " 'делаешь',\n",
       " 'умеренными',\n",
       " 'весами',\n",
       " 'идеальной',\n",
       " 'техникой',\n",
       " 'без',\n",
       " 'сильных',\n",
       " 'прогибов',\n",
       " 'пояснице',\n",
       " 'тренировки',\n",
       " 'начинай',\n",
       " 'активации',\n",
       " 'мышц',\n",
       " 'кора',\n",
       " 'всякие',\n",
       " 'планки',\n",
       " 'вакуумы',\n",
       " 'работа',\n",
       " 'на',\n",
       " 'фитболе',\n",
       " 'это',\n",
       " 'самые',\n",
       " 'важные',\n",
       " 'мышцы',\n",
       " 'только',\n",
       " 'потом',\n",
       " 'приступай',\n",
       " 'железу',\n",
       " 'обрати',\n",
       " 'внимание',\n",
       " 'на',\n",
       " 'свою',\n",
       " 'осанку',\n",
       " 'если',\n",
       " 'скругленная',\n",
       " 'спина',\n",
       " 'как',\n",
       " 'большинства',\n",
       " 'то',\n",
       " 'будешь',\n",
       " 'многие',\n",
       " 'упражнения',\n",
       " 'выполнять',\n",
       " 'не',\n",
       " 'правильно',\n",
       " 'даже',\n",
       " 'травмоопасно',\n",
       " 'например',\n",
       " 'тяги',\n",
       " 'будешь',\n",
       " 'делать',\n",
       " 'за',\n",
       " 'счет',\n",
       " 'рук',\n",
       " 'не',\n",
       " 'спины',\n",
       " 'для',\n",
       " 'начала',\n",
       " 'надо',\n",
       " 'расскрыть',\n",
       " 'грудную',\n",
       " 'клетку',\n",
       " 'гугли',\n",
       " 'специальные',\n",
       " 'упражнения',\n",
       " 'также',\n",
       " 'поможет',\n",
       " 'миофасциальный',\n",
       " 'релиз',\n",
       " 'работа',\n",
       " 'на',\n",
       " 'спец',\n",
       " 'валике',\n",
       " 'отведения',\n",
       " 'спасибо',\n",
       " 'анон',\n",
       " 'добра',\n",
       " 'тебя',\n",
       " 'няша',\n",
       " 'упор',\n",
       " 'на',\n",
       " 'мышцы',\n",
       " 'спины',\n",
       " 'сделаешь',\n",
       " 'каркас',\n",
       " 'можешь',\n",
       " 'потихоничку',\n",
       " 'вносить',\n",
       " 'разнообразия',\n",
       " 'упражнения',\n",
       " 'для',\n",
       " 'спины',\n",
       " 'все',\n",
       " 'выше',\n",
       " 'сказано',\n",
       " 'трапеции',\n",
       " 'то',\n",
       " 'блядь',\n",
       " 'хоть',\n",
       " 'как',\n",
       " 'то',\n",
       " 'можно',\n",
       " 'бомбить',\n",
       " 'самого',\n",
       " 'сколиоз',\n",
       " 'тренер',\n",
       " 'заставлял',\n",
       " 'делать',\n",
       " 'фуллбади',\n",
       " 'гиперэкстензия',\n",
       " 'тренажёры',\n",
       " 'на',\n",
       " 'разгибание',\n",
       " 'ног',\n",
       " 'сидя',\n",
       " 'сгибание',\n",
       " 'ног',\n",
       " 'лёжа',\n",
       " 'тяга',\n",
       " 'верхнего',\n",
       " 'блока',\n",
       " 'за',\n",
       " 'спину',\n",
       " 'брусья',\n",
       " 'турник',\n",
       " 'жим',\n",
       " 'лёжа',\n",
       " 'пресс',\n",
       " 'ну',\n",
       " 'кардио',\n",
       " 'не',\n",
       " 'на',\n",
       " 'беговой',\n",
       " 'дорожке',\n",
       " 'на',\n",
       " 'велосипеде',\n",
       " 'по',\n",
       " 'минут',\n",
       " 'до',\n",
       " 'после',\n",
       " 'тренировки',\n",
       " 'разве',\n",
       " 'гиперэкстезию',\n",
       " 'можно',\n",
       " 'хотя',\n",
       " 'жим',\n",
       " 'стоя',\n",
       " 'хрен',\n",
       " 'чем',\n",
       " 'заменишь',\n",
       " 'нахрена',\n",
       " 'он',\n",
       " 'вообще',\n",
       " 'нужен',\n",
       " 'op',\n",
       " 'реально',\n",
       " 'ли',\n",
       " 'вообще',\n",
       " 'нарастить',\n",
       " 'мышечную',\n",
       " 'массу',\n",
       " 'без',\n",
       " 'нагрузки',\n",
       " 'на',\n",
       " 'позвоночный',\n",
       " 'столб',\n",
       " 'интересуюсь',\n",
       " 'местных',\n",
       " 'экспертов',\n",
       " 'бамр',\n",
       " 'пока',\n",
       " 'не',\n",
       " 'утонул',\n",
       " 'есть',\n",
       " 'тут',\n",
       " 'такие',\n",
       " 'аноны',\n",
       " 'можно',\n",
       " 'ли',\n",
       " 'накачаться',\n",
       " 'без',\n",
       " 'осевых',\n",
       " 'удваиваю',\n",
       " 'прошлый',\n",
       " 'вопрос',\n",
       " 'бамп',\n",
       " 'последний',\n",
       " 'можно',\n",
       " 'можно']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "test = []\n",
    "\n",
    "it = 0\n",
    "for topic in threads_by_topic:\n",
    "    for thread in topic:\n",
    "        full = []\n",
    "        for post in thread:\n",
    "            full.extend(post)\n",
    "        it = it + 1\n",
    "        if(it % 10 == 0):\n",
    "            test.append(full)\n",
    "        else:\n",
    "            data.append(full)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание.}$\n",
    "В русском языке есть множество слов (частицы, междометия, всё что вы хотите), которые никак не отображают смысл слов и являются вспомогательными. Чтобы ваша модель работала лучше -- добавьте стоп-слова в список RUSSIAN_STOP_WORDS или в строку st_str. Эти слова отфильтруются из датасета перед тем, как модель начнет обучаться на датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "\n",
    "RUSSIAN_STOP_WORDS = ['не', 'это', 'лишь', 'поэтому' 'что','чем','это','как','https','нет','op','он','же','так','но','да','нет','или','и', 'на', \"то\", \"бы\", \"все\", \"ты\", \"если\", \"по\", \"за\", \"там\", \"ну\", \"уже\", \"от\", \"есть\",\"был\", \"даже\", \"было\", \"www\", \"com\", \"youtube\", \"из\", \"будет\", \"mp\", \"они\", \"только\", \"его\", \"она\", \"вот\", 'просто', 'watch', 'кто', 'для', 'когда', 'тут', 'мне', 'где', 'мы', 'какой', 'может', 'меня', 'до', 'про', 'http', 'раз', 'почему', 'тебя', 'ещё', 'их', 'сейчас', 'тоже', 'во', 'чтобы', 'этого','без', 'него','вы','такой', 'можно', 'надо', 'нахуй', 'ли', 'потом', 'тред', 'больше', 'лучше', 'хуй', 'сам', 'после', 'со', 'лол', 'быть', 'нужно', 'этом', 'блять', 'бля', 'того', 'ничего', 'потому', 'нибудь', 'этот', 'под', 'через', 'ни', 'себе', 'ему', 'при', 'какие', 'пиздец', 'теперь', 'хоть', 'говно', 'тогда', 'блядь', 'кстати', 'че', 'себя', 'конечно', 'типа', 'много', 'том', 'нихуя', 'куда', 'всегда', 'нас', 'тот', 'ведь', 'эти', 'них', 'сука', 'пока', 'более', 'чего', 'html', 'были', 'всех', 'была', 'например', 'тем', 'ru', 'зачем', 'либо', 'вроде', 'всего', 'вопрос', 'php', 'против', 'здесь', 'ее', 'значит', 'совсем', 'сколько', 'им', 'org', 'именно', 'эту',]\n",
    "st_str = \"которых которые твой которой которого сих ком свой твоя этими слишком нами всему будь саму чаще ваше сами наш затем еще самих наши ту каждое мочь весь этим наша своих оба который зато те этих вся ваш такая теми ею которая нередко каждая также чему собой самими нем вами ими откуда такие тому та очень сама нему алло оно этому кому тобой таки твоё каждые твои мой нею самим ваши ваша кем мои однако сразу свое ними всё неё тех хотя всем тобою тебе одной другие этао само эта буду самой моё своей такое всею будут своего кого свои мог нам особенно её самому наше кроме вообще вон мною никто это\"\n",
    "RUSSIAN_STOP_WORDS.extend(st_str.split(' '))\n",
    "\n",
    "data = [list(filter(lambda word: not word in RUSSIAN_STOP_WORDS, piece)) for piece in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь и на его основе преобразуем слова в их id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим LDA-модель, используя библиотеку gensim. Зададим число тем равно числу скачанных досок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "model = LdaModel(corpus, id2word=id2word, num_topics=len(threads_by_topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь получим топ-10 самых используемых в каждой теме слов.\n",
    "\n",
    "$\\textbf{Задание.}$\n",
    "Оцените насколько хорошо модель разделила темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['день', 'кг', 'бог', 'один', 'делать', 'лет', 'жизни', 'время', 'человек', 'люди']\n",
      "['кг', 'день', 'человек', 'бог', 'можешь', 'время', 'делать', 'бога', 'жизни', 'людей']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(threads_by_topic)):\n",
    "    print([id2word[id[0]] for id in model.get_topic_terms(topicid = i, topn = 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь прогоним тестовые треды на модели. Тестовый датасет разделен на n равных частей по 20 тредов, i-ая соответствует i-й доске."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.21614233), (1, 0.7838577)]\n"
     ]
    }
   ],
   "source": [
    "other_corpus = [id2word.doc2bow(text) for text in [list(filter(lambda word: not word in RUSSIAN_STOP_WORDS, piece)) for piece in test]]\n",
    "\n",
    "vector = [model[unseen_doc] for unseen_doc in other_corpus]\n",
    "print(vector[0]) #вероятности принадлежности 0-го тестового треда в ту или иную тему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text #0, topic #1, prob = 0.7838577\n",
      "Text #1, topic #1, prob = 0.7610595\n",
      "Text #2, topic #0, prob = 0.56745833\n",
      "Text #3, topic #1, prob = 0.5545186\n",
      "Text #4, topic #0, prob = 0.6075075\n",
      "Text #5, topic #0, prob = 0.55626833\n",
      "Text #6, topic #1, prob = 0.53630984\n",
      "Text #7, topic #1, prob = 0.55875\n",
      "Text #8, topic #0, prob = 0.86626035\n",
      "Text #9, topic #1, prob = 0.5035742\n",
      "Text #10, topic #0, prob = 0.58734775\n",
      "Text #11, topic #1, prob = 0.7281541\n",
      "Text #12, topic #1, prob = 0.7851186\n",
      "Text #13, topic #0, prob = 0.5630218\n",
      "Text #14, topic #0, prob = 0.60416245\n",
      "Text #15, topic #0, prob = 0.55250865\n",
      "Text #16, topic #0, prob = 0.68980014\n",
      "Text #17, topic #0, prob = 0.7368434\n",
      "Text #18, topic #1, prob = 0.75275594\n",
      "Text #19, topic #0, prob = 0.6599304\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for res in vector:\n",
    "    max_it = 0\n",
    "    if(len(res) > 0):\n",
    "        for it in range(1, len(res)):\n",
    "            if(res[max_it][1] < res[it][1]):\n",
    "                max_it = it\n",
    "        print(\"Text #\" + str(i) + \", topic #\" + str(max_it) + str(\", prob = \" + str(res[max_it][1])))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание.}$\n",
    "\n",
    "Оцените результаты работы модели на тест сете. Если модель разделили данные плохо -- объясните, почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. А теперь нормальный датасет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте воспользуемся более стандартным датасетом библиотеки sklreatn -- 20newsgroups, посвященную статьям на различные темы. Выберем 6 -- Атеизм, яблочное железо, автомобили, хоккей, космос, христианство, ближний восток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'rec.autos',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.mideast']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories = categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание}$\n",
    "\n",
    "Найдите библиотечный или опишите свой список ENGSLISH_STOP_WORDS, убирающий не несущие никакого смысла английские слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "\n",
    "ENGLISH_STOP_WORDS = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\", \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\", \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]\n",
    "data = [list(filter(lambda word: not word in ENGLISH_STOP_WORDS, simple_preprocess(piece))) for piece in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Большое задание 1.}$\n",
    "\n",
    "Для списка data создайте словарь id2word. Получите преобразованный TermDocumentFrequency список corpust и обучите на нем LDA модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 4), (10, 2), (11, 6), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel, LsiModel\n",
    "\n",
    "id2word = corpora.Dictionary(data)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "model = LdaModel(corpus, id2word=id2word, num_topics=len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people', 'bible', 'religious', 'laws', 'catholic', 'smileys', 'representation', 'christian', 'safest', 'point']\n",
      "['chart', 'failure', 'psuvm', 'prevention', 'space', 'pregnant', 'rate', 'framework', 'list', 'public']\n",
      "['offended', 'pontiac', 'fingers', 'failure', 'cares', 'fellow', 'rate', 'list', 'school', 'chart']\n",
      "['people', 'bible', 'point', 'turkey', 'kurds', 'years', 'armenian', 'reasonable', 'russian', 'source']\n",
      "['tony', 'rate', 'abstinence', 'failure', 'sex', 'lindros', 'ottawa', 'binghamton', 'warriors', 'sarcasm']\n",
      "['finnish', 'team', 'lindros', 'gt', 'manta', 'govern', 'pretty', 'rangers', 'american', 'washington']\n",
      "['blues', 'game', 'hawks', 'playoffs', 'thankfully', 'predicted', 'face', 'place', 'day', 'left']\n"
     ]
    }
   ],
   "source": [
    "#Выведем получившийся список тем:\n",
    "for i in range(len(categories)):\n",
    "    print([id2word[id[0]] for id in model.get_topic_terms(topicid = i, topn = 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Большое задание 2.}$\n",
    "\n",
    "В соответствии с тренировочными, обработайте тестовые данные.\n",
    "\n",
    "Напишите функцию, которая с помощью модели возвращает наиболее вероятный id темы. С помощью F-меры оцените правильность работы модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories = categories)\n",
    "\n",
    "other_corpus = [id2word.doc2bow(text) for text in [list(filter(lambda word: not word in ENGLISH_STOP_WORDS, simple_preprocess(piece))) for piece in newsgroups_test.data]]\n",
    "vector = [model[unseen_doc] for unseen_doc in other_corpus]\n",
    "\n",
    "#TODO: YOUD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text #3, topic #1, prob = 0.43598503\n"
     ]
    }
   ],
   "source": [
    "def probability_of_text(text):\n",
    "    res = vector[text]\n",
    "    max_it = 0\n",
    "    if(len(res) > 0):\n",
    "        for it in range(1, len(res)):\n",
    "            if(res[max_it][1] < res[it][1]):\n",
    "                max_it = it\n",
    "        print(\"Text #\" + str(text) + \", topic #\" + str(max_it) + str(\", prob = \" + str(res[max_it][1])))\n",
    "    \n",
    "probability_of_text(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.18260217472815898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "i = 0\n",
    "vector_pred = []\n",
    "for res in vector:\n",
    "    max_it = 0\n",
    "    if(len(res) > 0):\n",
    "        for it in range(1, len(res)):\n",
    "            if(res[max_it][1] < res[it][1]):\n",
    "                max_it = it\n",
    "        vector_pred.append(max_it)\n",
    "    i = i + 1\n",
    "\n",
    "y_true = newsgroups_test.target\n",
    "y_pred = vector_pred\n",
    "print(f'f1-score: {f1_score(y_true, y_pred, average=\"micro\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправление номеров тем в данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 6 4 1 5 1 6 3 5 0 6 4 6 3 5 6 0 6 2]\n",
      "[0, 0, 3, 1, 0, 0, 0, 0, 2, 0, 0, 2, 6, 1, 2, 0, 1, 1, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_true[:20])\n",
    "print(y_pred[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "permuts = list(itertools.permutations([0,1,2,3,4,5,6]))\n",
    "real_topics = newsgroups_test['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for per in permuts: \n",
    "    new_list = []\n",
    "    for items in y_pred:\n",
    "        new_list.append(per[items])    \n",
    "    res[per] = f1_score(real_topics, new_list, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1, 2, 6, 0, 3, 4)\n",
      "0.24109486314210724\n"
     ]
    }
   ],
   "source": [
    "max_key = max(res, key=res.get)\n",
    "print(max_key)\n",
    "print(res[(5, 1, 2, 6, 0, 3, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 1, 1, 1, 1, 2, 0, 2, 2, 0, 0, 2, 1, 6, 0, 1, 1, 3, 1]\n",
      "[5, 2, 1, 1, 1, 1, 2, 5, 2, 2, 5, 5, 2, 1, 4, 5, 1, 1, 6, 1]\n",
      "[5 0 6 4 1 5 1 6 3 5 0 6 4 6 3 5 6 0 6 2]\n"
     ]
    }
   ],
   "source": [
    "y_pred = vector_pred\n",
    "\n",
    "print(y_pred[:20])\n",
    "for index, items in enumerate(y_pred):\n",
    "    if items == 0:\n",
    "        y_pred[index] = 5\n",
    "    elif items == 1:\n",
    "        y_pred[index] = 1\n",
    "    elif items == 2:\n",
    "        y_pred[index] = 2\n",
    "    elif items == 3:\n",
    "        y_pred[index] = 6\n",
    "    elif items == 4:\n",
    "        y_pred[index] = 0\n",
    "    elif items == 5:\n",
    "        y_pred[index] = 3\n",
    "    elif items == 6:\n",
    "        y_pred[index] = 4\n",
    "\n",
    "print(y_pred[:20])\n",
    "print(y_true[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.24109486314210724\n"
     ]
    }
   ],
   "source": [
    "print(f'f1-score: {f1_score(y_true, y_pred, average=\"micro\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
